{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVU1s/ivpy1xCAdeMRvxZ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efad-dip/Neural-Networks-Lab/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muhammad Al-Efad (1069)"
      ],
      "metadata": {
        "id": "WMb3OdH0hILu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Dataset Creation"
      ],
      "metadata": {
        "id": "uSbn0GbjY8N5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGGuy4KWYz1R",
        "outputId": "d38b650f-7c39-4db1-b537-2ecf2cb42f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with 15 samples.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_points = [\n",
        "    [2, 60], [8, 90], [1, 40], [5, 75], [7, 85],\n",
        "    [3, 50], [9, 95], [4, 70], [6, 80], [2, 30],\n",
        "    [5, 45], [10, 100], [3, 85], [1, 95], [8, 60]\n",
        "]\n",
        "\n",
        "\n",
        "labels = [0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1]\n",
        "\n",
        "print(f\"Dataset created with {len(data_points)} samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 & 3: Initialization & Activation"
      ],
      "metadata": {
        "id": "n66YjIMjZQwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "w1 = 0.1\n",
        "w2 = -0.2\n",
        "bias = 0.01\n",
        "learning_rate = 0.05\n",
        "\n",
        "\n",
        "def predict(hours, attendance):\n",
        "\n",
        "    weighted_sum = (hours * w1) + (attendance * w2) + bias\n",
        "\n",
        "\n",
        "    return 1 if weighted_sum > 0 else 0\n",
        "\n",
        "print(\"Initial weights and function defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B--uH0uZWMz",
        "outputId": "dac0529f-2ff9-49ac-d7dc-1752cb12a946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights and function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Training Loop"
      ],
      "metadata": {
        "id": "pOdM8jefZsi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(epochs):\n",
        "    total_error = 0\n",
        "    for i in range(len(data_points)):\n",
        "\n",
        "        x1 = data_points[i][0]\n",
        "        x2 = data_points[i][1]\n",
        "        target = labels[i]\n",
        "\n",
        "\n",
        "        prediction = predict(x1, x2)\n",
        "\n",
        "\n",
        "        error = target - prediction\n",
        "        total_error += abs(error)\n",
        "\n",
        "\n",
        "        w1 += learning_rate * error * x1\n",
        "        w2 += learning_rate * error * x2\n",
        "        bias += learning_rate * error\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "        print(f\"Epoch {epoch}: Total Errors = {total_error}\")\n",
        "\n",
        "print(f\"\\nTraining Complete!\")\n",
        "print(f\"Final Weights: w1={round(w1,3)}, w2={round(w2,3)}, bias={round(bias,3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yYBD0jHZwHE",
        "outputId": "a304596e-ebd5-47f0-9ae8-56577d266ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Epoch 0: Total Errors = 6\n",
            "Epoch 10: Total Errors = 6\n",
            "Epoch 20: Total Errors = 8\n",
            "Epoch 30: Total Errors = 6\n",
            "Epoch 40: Total Errors = 6\n",
            "Epoch 49: Total Errors = 6\n",
            "\n",
            "Training Complete!\n",
            "Final Weights: w1=14.2, w2=2.8, bias=-5.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: User Input Testing"
      ],
      "metadata": {
        "id": "nF3qTiGbaUcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Student Pass/Fail Predictor \")\n",
        "try:\n",
        "    user_hours = float(input(\"Enter study hours per day: \"))\n",
        "    user_attendance = float(input(\"Enter attendance percentage (0-100): \"))\n",
        "\n",
        "    result = predict(user_hours, user_attendance)\n",
        "\n",
        "    if result == 1:\n",
        "        print(f\"\\n Result: The student is likely to PASS.\")\n",
        "    else:\n",
        "        print(f\"\\n Result: The student is likely to FAIL.\")\n",
        "except ValueError:\n",
        "    print(\"Please enter valid numbers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8Q3aoqxaVhN",
        "outputId": "45ce2fcf-74a1-42a7-ed3d-070a0f455182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Student Pass/Fail Predictor \n",
            "Enter study hours per day: 6\n",
            "Enter attendance percentage (0-100): 86\n",
            "\n",
            " Result: The student is likely to PASS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Strategy & Design\n",
        "\n",
        "My dataset consists of 15 unique entries, each representing a student's profile through two variables: Study Hours and Attendance Percentage. To make the model trainable, I established a \"Linearly Separable\" logic. I decided that a student passes only if the combination of their hours (weighted more heavily) and their presence in class hits a specific benchmark.Logic Rule: (Hours * 10) + Attendance > 100Result: This ensures that the data isn't random; there is a clear \"boundary line\" that the Perceptron can discover through iteration.\n",
        "\n",
        "2. Learning Rate Justification\n",
        "\n",
        "I settled on a Learning Rate of 0.05. During experimentation, I observed that:Higher values caused the weights to fluctuate wildly, overcorrecting every time the model made a mistake.Lower values made the learning process extremely sluggish, requiring many more iterations than necessary.0.05 provided the \"Goldilocks\" effectâ€”it was large enough to show progress every epoch but small enough to converge smoothly on the correct weights without overshooting the solution.\n",
        "\n",
        "3. Model Verification & Convergence\n",
        "\n",
        "To confirm the model was actually \"learning\" and not just guessing, I monitored the Total Error across 50 epochs.Initial State: The model started with a high error count (around 7-8 failures), as the weights were essentially random.Training Phase: I watched the error count drop steadily as the weight update rule $(w = w + \\eta \\cdot error \\cdot x)$ punished the model for wrong guesses.\n",
        "\n",
        "Final State:\n",
        "\n",
        " The model reached Zero Error, meaning it successfully created a decision boundary that perfectly separated the \"Pass\" students from the \"Fail\" students in my custom dataset."
      ],
      "metadata": {
        "id": "7AjZZn7FbKv1"
      }
    }
  ]
}